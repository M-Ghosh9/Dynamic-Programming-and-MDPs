# -*- coding: utf-8 -*-
"""02451677coursework_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qQhim-rdaEl0A31Mb-9LHkmOLXLCB1do
"""

import numpy as np
import random
import matplotlib.pyplot as plt # Graphical library
from sklearn.metrics import mean_squared_error # Mean-squared error function
import sys
import itertools

"""# Coursework 1 :
See pdf for instructions.
"""

# WARNING: fill in these two functions that will be used by the auto-marking script
# [Action required]

def get_CID():
  return "02451677" # Return your CID (add 0 at the beginning to ensure it is 8 digits long)

def get_login():
  return "Mg423" # Return your short imperial login

"""## Helper class"""

# This class is used ONLY for graphics
# YOU DO NOT NEED to understand it to work on this coursework

class GraphicsMaze(object):

  def __init__(self, shape, locations, default_reward, obstacle_locs, absorbing_locs, absorbing_rewards, absorbing):

    self.shape = shape
    self.locations = locations
    self.absorbing = absorbing

    # Walls
    self.walls = np.zeros(self.shape)
    for ob in obstacle_locs:
      self.walls[ob] = 20

    # Rewards
    self.rewarders = np.ones(self.shape) * default_reward
    for i, rew in enumerate(absorbing_locs):
      self.rewarders[rew] = 10 if absorbing_rewards[i] > 0 else -10

    # Print the map to show it
    self.paint_maps()

  def paint_maps(self):
    """
    Print the Maze topology (obstacles, absorbing states and rewards)
    input: /
    output: /
    """
    plt.figure(figsize=(15,10))
    plt.imshow(self.walls + self.rewarders)
    plt.show()

  def paint_state(self, state):
    """
    Print one state on the Maze topology (obstacles, absorbing states and rewards)
    input: /
    output: /
    """
    states = np.zeros(self.shape)
    states[state] = 30
    plt.figure(figsize=(15,10))
    plt.imshow(self.walls + self.rewarders + states)
    plt.show()

  def draw_deterministic_policy(self, Policy):
    """
    Draw a deterministic policy
    input: Policy {np.array} -- policy to draw (should be an array of values between 0 and 3 (actions))
    output: /
    """
    plt.figure(figsize=(15,10))
    plt.imshow(self.walls + self.rewarders) # Create the graph of the Maze
    for state, action in enumerate(Policy):
      if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any action
        continue
      arrows = [r"$\uparrow$",r"$\rightarrow$", r"$\downarrow$", r"$\leftarrow$"] # List of arrows corresponding to each possible action
      action_arrow = arrows[action] # Take the corresponding action
      location = self.locations[state] # Compute its location on graph
      plt.text(location[1], location[0], action_arrow, ha='center', va='center') # Place it on graph
    plt.show()

  def draw_policy(self, Policy):
    """
    Draw a policy (draw an arrow in the most probable direction)
    input: Policy {np.array} -- policy to draw as probability
    output: /
    """
    deterministic_policy = np.array([np.argmax(Policy[row]) for row in range(Policy.shape[0])])
    self.draw_deterministic_policy(deterministic_policy)

  def draw_value(self, Value):
    """
    Draw a policy value
    input: Value {np.array} -- policy values to draw
    output: /
    """
    plt.figure(figsize=(15,10))
    plt.imshow(self.walls + self.rewarders) # Create the graph of the Maze
    for state, value in enumerate(Value):
      if(self.absorbing[0, state]): # If it is an absorbing state, don't plot any value
        continue
      location = self.locations[state] # Compute the value location on graph
      plt.text(location[1], location[0], np.round(value,2), ha='center', va='center') # Place it on graph
    plt.show()

  def draw_deterministic_policy_grid(self, Policies, title, n_columns, n_lines):
    """
    Draw a grid representing multiple deterministic policies
    input: Policies {np.array of np.array} -- array of policies to draw (each should be an array of values between 0 and 3 (actions))
    output: /
    """
    plt.figure(figsize=(20,8))
    for subplot in range (len(Policies)): # Go through all policies
      ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each policy
      ax.imshow(self.walls+self.rewarders) # Create the graph of the Maze
      for state, action in enumerate(Policies[subplot]):
        if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any action
          continue
        arrows = [r"$\uparrow$",r"$\rightarrow$", r"$\downarrow$", r"$\leftarrow$"] # List of arrows corresponding to each possible action
        action_arrow = arrows[action] # Take the corresponding action
        location = self.locations[state] # Compute its location on graph
        plt.text(location[1], location[0], action_arrow, ha='center', va='center') # Place it on graph
      ax.title.set_text(title[subplot]) # Set the title for the graph given as argument
    plt.show()

  def draw_policy_grid(self, Policies, title, n_columns, n_lines):
    """
    Draw a grid representing multiple policies (draw an arrow in the most probable direction)
    input: Policy {np.array} -- array of policies to draw as probability
    output: /
    """
    deterministic_policies = np.array([[np.argmax(Policy[row,:]) for row in range(Policy.shape[0])] for Policy in Policies])
    self.draw_deterministic_policy_grid(deterministic_policies, title, n_columns, n_lines)

  def draw_value_grid(self, Values, title, n_columns, n_lines):
    """
    Draw a grid representing multiple policy values
    input: Values {np.array of np.array} -- array of policy values to draw
    output: /
    """
    plt.figure(figsize=(20,8))
    for subplot in range (len(Values)): # Go through all values
      ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each value
      ax.imshow(self.walls+self.rewarders) # Create the graph of the Maze
      for state, value in enumerate(Values[subplot]):
        if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any value
          continue
        location = self.locations[state] # Compute the value location on graph
        plt.text(location[1], location[0], round(value,1), ha='center', va='center') # Place it on graph
      ax.title.set_text(title[subplot]) # Set the title for the graoh given as argument
    plt.show()

"""## Maze class"""

# This class define the Maze environment

class Maze(object):

  # [Action required]
  def __init__(self):
    """
    Maze initialisation.
    input: /
    output: /
    """

    # [Action required]
    # Properties set from the CID
    self._prob_success = 0.84 # float
    self._gamma = 0.9 # float
    self._goal = 3 # integer (0 for R0, 1 for R1, 2 for R2, 3 for R3)

    # Build the maze
    self._build_maze()

  # Functions used to build the Maze environment
  # You DO NOT NEED to modify them
  def _build_maze(self):
    """
    Maze initialisation.
    input: /
    output: /
    """

    # Properties of the maze
    self._shape = (13, 10)
    self._obstacle_locs = [
                          (1,0), (1,1), (1,2), (1,3), (1,4), (1,7), (1,8), (1,9), \
                          (2,1), (2,2), (2,3), (2,7), \
                          (3,1), (3,2), (3,3), (3,7), \
                          (4,1), (4,7), \
                          (5,1), (5,7), \
                          (6,5), (6,6), (6,7), \
                          (8,0), \
                          (9,0), (9,1), (9,2), (9,6), (9,7), (9,8), (9,9), \
                          (10,0)
                         ] # Location of obstacles
    self._absorbing_locs = [(2,0), (2,9), (10,1), (12,9)] # Location of absorbing states
    self._absorbing_rewards = [ (500 if (i == self._goal) else -50) for i in range (4) ]
    self._starting_locs = [(0,0), (0,1), (0,2), (0,3), (0,4), (0,5), (0,6), (0,7), (0,8), (0,9)] #Reward of absorbing states
    self._default_reward = -1 # Reward for each action performs in the environment
    self._max_t = 500 # Max number of steps in the environment

    # Actions
    self._action_size = 4
    self._direction_names = ['N','E','S','W'] # Direction 0 is 'N', 1 is 'E' and so on

    # States
    self._locations = []
    for i in range (self._shape[0]):
      for j in range (self._shape[1]):
        loc = (i,j)
        # Adding the state to locations if it is no obstacle
        if self._is_location(loc):
          self._locations.append(loc)
    self._state_size = len(self._locations)

    # Neighbours - each line is a state, ranked by state-number, each column is a direction (N, E, S, W)
    self._neighbours = np.zeros((self._state_size, 4))

    for state in range(self._state_size):
      loc = self._get_loc_from_state(state)

      # North
      neighbour = (loc[0]-1, loc[1]) # North neighbours location
      if self._is_location(neighbour):
        self._neighbours[state][self._direction_names.index('N')] = self._get_state_from_loc(neighbour)
      else: # If there is no neighbour in this direction, coming back to current state
        self._neighbours[state][self._direction_names.index('N')] = state

      # East
      neighbour = (loc[0], loc[1]+1) # East neighbours location
      if self._is_location(neighbour):
        self._neighbours[state][self._direction_names.index('E')] = self._get_state_from_loc(neighbour)
      else: # If there is no neighbour in this direction, coming back to current state
        self._neighbours[state][self._direction_names.index('E')] = state

      # South
      neighbour = (loc[0]+1, loc[1]) # South neighbours location
      if self._is_location(neighbour):
        self._neighbours[state][self._direction_names.index('S')] = self._get_state_from_loc(neighbour)
      else: # If there is no neighbour in this direction, coming back to current state
        self._neighbours[state][self._direction_names.index('S')] = state

      # West
      neighbour = (loc[0], loc[1]-1) # West neighbours location
      if self._is_location(neighbour):
        self._neighbours[state][self._direction_names.index('W')] = self._get_state_from_loc(neighbour)
      else: # If there is no neighbour in this direction, coming back to current state
        self._neighbours[state][self._direction_names.index('W')] = state

    # Absorbing
    self._absorbing = np.zeros((1, self._state_size))
    for a in self._absorbing_locs:
      absorbing_state = self._get_state_from_loc(a)
      self._absorbing[0, absorbing_state] = 1

    # Transition matrix
    self._T = np.zeros((self._state_size, self._state_size, self._action_size)) # Empty matrix of domension S*S*A
    for action in range(self._action_size):
      for outcome in range(4): # For each direction (N, E, S, W)
        # The agent has prob_success probability to go in the correct direction
        if action == outcome:
          prob = 1 - 3.0 * ((1.0 - self._prob_success) / 3.0) # (theoritically equal to self.prob_success but avoid rounding error and garanty a sum of 1)
        # Equal probability to go into one of the other directions
        else:
          prob = (1.0 - self._prob_success) / 3.0

        # Write this probability in the transition matrix
        for prior_state in range(self._state_size):
          # If absorbing state, probability of 0 to go to any other states
          if not self._absorbing[0, prior_state]:
            post_state = self._neighbours[prior_state, outcome] # Post state number
            post_state = int(post_state) # Transform in integer to avoid error
            self._T[prior_state, post_state, action] += prob

    # Reward matrix
    self._R = np.ones((self._state_size, self._state_size, self._action_size)) # Matrix filled with 1
    self._R = self._default_reward * self._R # Set default_reward everywhere
    for i in range(len(self._absorbing_rewards)): # Set absorbing states rewards
      post_state = self._get_state_from_loc(self._absorbing_locs[i])
      self._R[:,post_state,:] = self._absorbing_rewards[i]

    # Creating the graphical Maze world
    self._graphics = GraphicsMaze(self._shape, self._locations, self._default_reward, self._obstacle_locs, self._absorbing_locs, self._absorbing_rewards, self._absorbing)

    # Reset the environment
    self.reset()


  def _is_location(self, loc):
    """
    Is the location a valid state (not out of Maze and not an obstacle)
    input: loc {tuple} -- location of the state
    output: _ {bool} -- is the location a valid state
    """
    if (loc[0] < 0 or loc[1] < 0 or loc[0] > self._shape[0]-1 or loc[1] > self._shape[1]-1):
      return False
    elif (loc in self._obstacle_locs):
      return False
    else:
      return True


  def _get_state_from_loc(self, loc):
    """
    Get the state number corresponding to a given location
    input: loc {tuple} -- location of the state
    output: index {int} -- corresponding state number
    """
    return self._locations.index(tuple(loc))


  def _get_loc_from_state(self, state):
    """
    Get the state number corresponding to a given location
    input: index {int} -- state number
    output: loc {tuple} -- corresponding location
    """
    return self._locations[state]

  # Getter functions used only for DP agents
  # You DO NOT NEED to modify them
  def get_T(self):
    return self._T

  def get_R(self):
    return self._R

  def get_absorbing(self):
    return self._absorbing

  # Getter functions used for DP, MC and TD agents
  # You DO NOT NEED to modify them
  def get_graphics(self):
    return self._graphics

  def get_action_size(self):
    return self._action_size

  def get_state_size(self):
    return self._state_size

  def get_gamma(self):
    return self._gamma

  # Functions used to perform episodes in the Maze environment
  def reset(self):
    """
    Reset the environment state to one of the possible starting states
    input: /
    output:
      - t {int} -- current timestep
      - state {int} -- current state of the envionment
      - reward {int} -- current reward
      - done {bool} -- True if reach a terminal state / 0 otherwise
    """
    self._t = 0
    self._state = self._get_state_from_loc(self._starting_locs[random.randrange(len(self._starting_locs))])
    self._reward = 0
    self._done = False
    return self._t, self._state, self._reward, self._done

  def step(self, action):
    """
    Perform an action in the environment
    input: action {int} -- action to perform
    output:
      - t {int} -- current timestep
      - state {int} -- current state of the envionment
      - reward {int} -- current reward
      - done {bool} -- True if reach a terminal state / 0 otherwise
    """
    # Ensure the action is within the valid range
    if action < 0 or action >= self.get_action_size():
        raise ValueError("Invalid action. Action should be in the range [0, {}].".format(self.get_action_size() - 1))
    # If environment already finished, print an error
    if self._done or self._absorbing[0, self._state]:
      print("Please reset the environment")
      return self._t, self._state, self._reward, self._done

    # Drawing a random number used for probaility of next state
    probability_success = random.uniform(0,1)

    # Look for the first possible next states (so get a reachable state even if probability_success = 0)
    new_state = 0
    while self._T[self._state, new_state, action] == 0:
      new_state += 1
    assert self._T[self._state, new_state, action] != 0, "Selected initial state should be probability 0, something might be wrong in the environment."

    # Find the first state for which probability of occurence matches the random value
    total_probability = self._T[self._state, new_state, action]
    while (total_probability < probability_success) and (new_state < self._state_size-1):
     new_state += 1
     total_probability += self._T[self._state, new_state, action]
    assert self._T[self._state, new_state, action] != 0, "Selected state should be probability 0, something might be wrong in the environment."

    # Setting new t, state, reward and done
    self._t += 1
    self._reward = self._R[self._state, new_state, action]
    self._done = self._absorbing[0, new_state] or self._t > self._max_t
    self._state = new_state
    return self._t, self._state, self._reward, self._done

"""## DP Agent"""

# This class define the Dynamic Programing agent

class DP_agent(object):

  # [Action required]
  # WARNING: make sure this function can be called by the auto-marking script
  def solve(self,env, gamma=0.9, threshold=0.0001):
    """
    Solve a given Maze environment using Dynamic Programming
    input: env {Maze object} -- Maze to solve
    output:
      - policy {np.array} -- Optimal policy found to solve the given Maze environment
      - V {np.array} -- Corresponding value function
    """

    # Initialisation (can be edited)
    policy = np.zeros((env.get_state_size(), env.get_action_size()))
    V = np.zeros(env.get_state_size())

    assert (gamma <=1) and (gamma >= 0), "Discount factor should be in the range [0, 1]."
    # Initialisation of epoch
    epochs = 0
    delta = threshold # Setting value of delta to go through the first breaking condition

    while delta >= threshold:
       epochs += 1 # Increment the epoch by 1
       delta = 0 # Reinitialise delta value to 0

      # For each state in range of state size
       for prev_state in range(env.get_state_size()):

      # If an absorbing state is not encountered
        if not env.get_absorbing()[0, prev_state]:

          # Store the previous value for that state in v
          v = V[prev_state]

          # Compute the Q-value
          Q = np.zeros(4)
          for post_state in range(env.get_state_size()):
            Q += env.get_T()[prev_state, post_state,:] * (env.get_R()[prev_state, post_state, :] + gamma * V[post_state])

          # Set the new value to the maximum of Q-value
          V[prev_state]= np.max(Q)

          # Compute the new delta value
          delta = max(delta, np.abs(v - V[prev_state]))

    # Fill in the optimal policy when the loop is terminated
    policy = np.zeros((env.get_state_size(), env.get_action_size())) # Initialisation of policy
    for prev_state in range(env.get_state_size()):
      # Computation of the Q values
      Q = np.zeros(4)
      for post_state in range(env.get_state_size()):
        Q += env.get_T()[prev_state, post_state,:] * (env.get_R()[prev_state, post_state, :] + gamma * V[post_state])

      # The action that maximises the findings of the Q value= probability 1
      policy[prev_state, np.argmax(Q)] = 1


    return policy, V,epochs

"""## MC agent"""

#this class define the Monte Carlo agent

class MC_agent(object):
  # [Action required]
  # WARNING: make sure this function can be called by the auto-marking script
    """
    Solve a given Maze environment using Monte Carlo learning
    input: env {Maze object} -- Maze to solve
    output:
      - policy {np.array} -- Optimal policy found to solve the given Maze environment
      - values {list of np.array} -- List of successive value functions for each episode
      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
    """

    # Initialisation (can be edited)
    ####
    # Add your code here
    # WARNING: this agent only has access to env.reset() and env.step()
    # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
    ####

    def __init__(self):
        pass

    def generate_state_action_reward(self, policy, env):
        state_action_reward = []  # Initialize an empty episode to store state-action-reward tuples
        _, state, _, _ = env.reset()  # Reset the environment and get the initial state

        while True:
            # Choose an action based on the current policy and state (epsilon-greedy)
            epsilon = 0.1  # Epsilon value for epsilon-greedy policy
            if np.random.rand() < epsilon:
                action = np.random.randint(env.get_action_size())  # Exploration: choose a random action
            else:
                action = np.argmax(policy[state])  # Exploitation: choose the action with the highest probability

            # Take the chosen action, observe the next state, reward, and done flag
            _, next_state, reward, done = env.step(action)

            # Append the current state, action, and reward to the episode
            state_action_reward.append((state, action, reward))
            state = next_state  # Update the current state to the next state

            if done:
                break  # Break the loop if the episode ends

        return state_action_reward

    def updateQvalues(self, episode, Q, returns, n_actions, state_size, env):
        G = 0  # Initialize the return G to 0
        total_reward = 0  # Initialize the total reward for the episode
        for t in range(len(episode) - 1, -1, -1):
            state, action, reward = episode[t]  # Retrieve state, action, and reward
            G = env.get_gamma() * G + reward  # Update the return G with discounting
            total_reward += reward  # Increment the total reward

            # Check if the state-action pair (state, action) has occurred earlier in the episode
            if (state, action) in [(x[0], x[1]) for x in episode[0:t - 1]]:
                returns[(state, action)] += G  # Update the sum of returns for the state-action pair
                Q[state][action] = np.mean(returns[(state, action)])  # Update the Q-value for the state-action pair

        epsilon = 0.1  # Epsilon value for epsilon-greedy policy
        policy = self.epsilon_greedy(Q, epsilon, n_actions, state_size)  # Update the policy
        return policy, total_reward  # Return the updated policy and the total reward for the episode

    def epsilon_greedy(self, Q, epsilon, n_actions, state_size):
        policy = np.ones((state_size, n_actions)) * epsilon / n_actions  # Initialize a uniform epsilon-greedy policy

        for state in range(state_size):
            best_action = np.argmax(Q[state])  # Find the best action for the current state
            policy[state][best_action] += 1 - epsilon  # Make the best action more likely in the policy

        return policy

    def value_function(self, episodes, env):
        state_size = env.get_state_size()
        values = np.zeros(state_size)  # Initialize the state values to zero
        alpha = 0.1  # Set the learning rate alpha

        for episode in episodes:
            G = 0  # Initialize the return G to 0
            for t in range(len(episode) - 1, -1, -1):
                state, _, reward = episode[t]  # Retrieve state and reward
                G = env.get_gamma() * G + reward  # Update the return G with discounting

                values[state] += alpha * (G - values[state])  # Update the state value incrementally

        return values

    def solve(self, env, num_episodes=5000):
        n_actions = env.get_action_size()
        state_size = env.get_state_size()
        Q = np.random.rand(state_size, n_actions)  # Initialize Q-values with random values
        values = np.random.rand(state_size)  # Initialize state values with random values
        policy = np.random.rand(state_size, n_actions)  # Initialize a random policy
        total_rewards = np.zeros(num_episodes).tolist()  # Initialize a list to store total rewards for each episode
        k = 1  # Initialize k for GLIE method
        returns = np.zeros((state_size, n_actions))  # Initialize an array to store returns for state-action pairs
        episodes = []  # Initialize a list to store episodes

        for i in range(num_episodes):
            episode = self.generate_state_action_reward(policy, env)  # Generate an episode
            episodes.append(episode)  # Add the episode to the list of episodes

            # Update policy and total reward for the episode
            policy, total_rewards[i] = self.updateQvalues(episode, Q, returns, n_actions, state_size, env)
            k += 1  # Increment k for GLIE method

        V = [self.value_function(episodes, env)]  # Compute the value function using the episodes

        return policy, V, total_rewards  # Return the policy, v and total rewards calculated

"""## TD agent"""

# This class define the Temporal-Difference agent

class TD_agent(object):
    """
    Solve a given Maze environment using Temporal Difference learning
    input: env {Maze object} -- Maze to solve
    output:
      - policy {np.array} -- Optimal policy found to solve the given Maze environment
      - values {list of np.array} -- List of successive value functions for each episode
      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
    """
    def __init__(self, env, epsilon, alpha, episodes):
        self.env = env
        self.epsilon = epsilon
        self.alpha = alpha
        self.episodes = episodes
        self.state_size = env.get_state_size()
        self.action_size = env.get_action_size()
        self.Q = np.random.rand(env.get_state_size(), env.get_action_size())
        self.V = np.random.rand(env.get_state_size())
        self.policy = np.zeros((env.get_state_size(), env.get_action_size()))
        self.total_rewards = []
        self.values=[self.V]
        self.k=1 #GLIE Method- agent explores all state-action pairs sufficiently and converge to give optimal solution

    def greedy_epsilon(self,state):
          if np.random.rand() < self.epsilon:
             # Exploration: choose a random action
              return np.random.randint(self.action_size)
          else:
             # Exploitation: choose the action with the highest Q-value for the given state
              action = np.argmax(self.Q[state,:])
              return action

    def solve(self):
      total_reward=0
       # Loop for each episode
      for episode in range(self.episodes):
         self.epsilon=1/self.k
         # Initialize the environment and the starting state
         _,state,_,_ = self.env.reset()
         while True:
           # Choose an action using epsilon-greedy policy
          action = self.greedy_epsilon(state)
           # Take the chosen action, observe the reward and next state
          _, next_state, reward, done = self.env.step(action)

         # Update Q-value for the current state-action pair
          q_value = self.Q[state][action]
          nextQ_value = max(self.Q[next_state][a] for a in range(self.action_size))
          self.policy[state][action]=nextQ_value
          newQ_value = q_value + self.alpha * (reward + self.env.get_gamma()*nextQ_value - q_value)
          self.V[state]=self.V[state]+alpha*(reward+self.env.get_gamma()*self.V[next_state]-self.V[state])
          self.Q[state][action] = newQ_value
          total_reward+=reward

          # Move to the next state
          state = next_state
          self.k+=1
          if done:
            break
      self.total_rewards.append(total_reward)
      self.values.append(self.V)
      return self.policy, self.values, self.total_rewards

"""## Example main"""

## Example main (can be edited)
if __name__ == '__main__':
### Question 0: Defining the environment
  print("Creating the Maze:\n")
  maze = Maze()

  ### Question 1: Dynamic programming
  dp_agent = DP_agent()
  dp_policy, dp_value,epochs = dp_agent.solve(maze)
  print("Question 1.1-")
  print("Results of the DP agent:\n")
  print("The DP policy is-")
  print(dp_policy)
  print("The DP value is-")
  print(dp_value)

  # Plot value function for policy iteration
  print("Question 1.2-")
  #print("The value of the optimal policy computed using value iteration is:\n\n {}\n\n".format(V))
  print("The graphical representation of the value of the optimal policy computed using value iteration is:\n")
  maze.get_graphics().draw_value(dp_value)
  # Plot policy for value iteration
  print("The graphical representation of the optimal policy computed using value iteration is:\n")
  maze.get_graphics().draw_policy(dp_policy)
  # Plot number of epoch
  print("\nIt took {} epochs".format(epochs))

  #Question 1.3:  impact of gamma and p on the value iteration algorithm
  print("Question 1.3-")
  gamma_range = [0, 0.25, 0.3, 0.4,0.5] #-ve gamma is not considered as the discount factor is between 0,1
  epochs_list = []
  policies = []
  values = []
  titles = []

  # Use value iteration for each gamma value
  for gamma in gamma_range:
      dp_policy, dp_value,epoch = dp_agent.solve(maze,threshold=0.0001, gamma=gamma)
      epochs_list.append(epoch)
      policies.append(dp_policy)
      values.append(dp_value)
      titles.append("gamma = {}".format(gamma))

  # Plot the number of epochs vs gamma values
  print("Impact of gamma value on the number of epochs needed for the value iteration algorithm:\n")
  plt.figure()
  plt.plot(gamma_range, epochs_list)
  plt.xlabel("Gamma range")
  plt.ylabel("Number of epochs")
  plt.title("Impact of epochs on Gamma range")
  plt.show()

  # Print all value functions and policies for different values of gamma
  print("\nGraphical representation of the value function for each gamma:\n")
  maze.get_graphics().draw_value_grid(values, titles, 1, 6)

  print("\nGraphical representation of the policy for each gamma:\n")
  maze.get_graphics().draw_policy_grid(policies, titles, 1, 6)

  ### Question 2: Monte-Carlo learning
  # Define hyperparameters
  #epsilon = 0.2
  #learning_rate = 0.2
  #gamma = 0.9
  #episodes = 100

  # Create an agent and solve the environment
  mc_agent = MC_agent() # you can intialize them here or the function already ahs predefined values
                      #maze, epsilon, learning_rate, gamma, episodes
  mc_policy, mc_values, total_rewards = mc_agent.solve(maze)

  print("Question 2.1 - Monte Carlo Policy:")
  #Print the policy values
  print("Final policies")
  print(mc_policy)
  # Print the final value function
  print("Final Value Function:")
  print(mc_values[-1])
  # Print the total rewards obtained in each episode
  print("Total Rewards:")
  print(total_rewards)

  # Plot value function for policy iteration
  print("Question 2.2-")
  #print("The value of the optimal policy computed using value iteration is:\n\n {}\n\n".format(V))
  print("The graphical representation of the value of the optimal policy computed using Monte Carlo Control is:\n")
  maze.get_graphics().draw_value(mc_values[-1])
  # Plot policy for value iteration
  #print("\n\nThe optimal policy computed using value iteration is:\n\n {}\n\n".format(policy))
  print("The graphical representation of the optimal policy computed using Monte Carlo Control is:\n")
  maze.get_graphics().draw_policy(mc_policy)

  #Plot the learning curve of the total non-discounted sum of reward and number of episodes along with mean and standard deviation
  print("Question 2.3-")
  # Number of training runs
  num_runs = 25
  all_rewards=[]
  # Lists to store rewards for each runall_rewards = []
  # Training loop for multiple runs
  for run in range(num_runs):
      mc_policy, mc_values, total_rewards = mc_agent.solve(maze)
      all_rewards.append(total_rewards)
  # Calculate mean and standard deviation of rewards
  mean_rewards = np.mean(all_rewards, axis=0)
  std_dev = np.std(all_rewards, axis=0)
  # Plot the learning curve with mean and standard deviation
  plt.figure(figsize=(10, 6))
  plt.plot(mean_rewards, label="Mean Reward")
  plt.fill_between(range(len(mean_rewards)), mean_rewards - std_dev, mean_rewards + std_dev, alpha=0.2, label="Std Deviation")
  plt.xlabel("Number of Episodes")
  plt.ylabel("Total Non-Discounted Reward")
  plt.legend()
  plt.title("Learning Curve - MC Agent (25 Runs)")
  plt.grid(True)
  plt.show()

  ### Question 3: Temporal-Difference learning
  epsilon=0.1
  alpha=1.0
  learning_rate=0.1
  gamma=0.9
  episodes=1000
  discount_factor=1.0
  # Create an instance of the TD_agent and pass the environment as an argument
  td_agent = TD_agent(maze,epsilon,alpha, episodes)
  # Now call the solve method
  td_policy, td_values, total_rewards = td_agent.solve()
  print("Results of the TD agent:\n")
  #Print the policy values
  print("Final policies", td_policy)
  # Print the final value function
  print("Final Value Function:", td_values)
  # Print the total rewards obtained in each episode
  print("The total rewards are-", total_rewards)

  # Plot value function for policy iteration
  print("Question 3.2-")
  print("The graphical representation of the value of the optimal values computed using Q-Learning is:\n")
  maze.get_graphics().draw_value(td_values[-1])

  # Plot policy for value iteration
  print("The graphical representation of the optimal policy computed using Q-Learning is:\n")
  maze.get_graphics().draw_policy(td_policy)

  print("Question 3.3- varyations between exploration parameters and learning curve")
  # Define a range of exploration parameters (epsilons) and learning rates (alphas)
  epsilons = [0.1, 0.2, 0.3, 0.4, 0.5]
  alphas = [0.1, 0.2, 0.3, 0.4, 0.5]
  rewards=[]
  # Create a grid of subplots to display the results
  fig, axs = plt.subplots(len(epsilons), len(alphas), figsize=(12, 12))
  # Iterate over different epsilon and alpha values
  for i, epsilon in enumerate(epsilons):
      for j, alpha in enumerate(alphas):
          # Create an instance of the TD_agent with the current epsilon and alpha
          td_agent = TD_agent(maze, epsilon, alpha, episodes)
          # Call the solve method to train the agent
          td_policy, td_values, total_rewards = td_agent.solve()
          # store the total_rewards in rewards=[]
          rewards.append(total_rewards)
          # Create a subplot for the current combination of epsilon and alpha
          ax = axs[i, j]
          ax.plot(rewards, label=f"Epsilon={epsilon}, Alpha={alpha}")
          ax.set_xlabel("Episodes")
          ax.set_ylabel("Total Rewards")
          ax.set_title(f"Epsilon={epsilon}, Alpha={alpha}")
          ax.legend()
  # Adjust spacing between subplots
  plt.tight_layout()
  # Display the plot
  plt.show()